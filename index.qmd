---
title: "Exam-01"
author: "Pablo Vivas"
date: "`r Sys.Date()`"
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 2
    code-copy: true
    code-fold: true
    highlight-style: nord
  docx: default
execute:
  warning: false
  error: false
  echo: true
theme: simplex
---

```{r}
#| label: set-up
#| echo: false

library(broom)
library(cowplot)
library(ggrepel)
library(gt)
library(skimr)
library(patchwork)
library(tidyverse)
```

# Question 1

## Descriptive analysis

First, we read the data and take a look at some observations.

```{r}
#| label: tbl-read-data
#| tbl-cap: Sample of (n=10) observations of the World data 

world = read_csv("data/world-3.csv")

world |> 
  sample_n(10) |> 
  arrange(country) |> 
  gt() |> 
  fmt_number(
    columns = c(6, 10, 11, 12),
    decimals = 2
  )
```

Then, we make a description of all the numeric values.

```{r}
#| label: tbl-desc-data
#| tbl-cap: Summary of numeric variables for the World data 

world |> 
  select(where(is.numeric)) |> 
  skim() |> 
  select(-1) |> 
  arrange(skim_variable) |> 
  gt() |> 
  fmt_number(
    columns = 3:10,
    decimals = 2
  ) |> 
  cols_label(
    skim_variable = "variable",
    n_missing = "missing",
    numeric.mean = "mean",
    numeric.sd = "std",
    numeric.p0 = "p0",
    numeric.p25 = "p25",
    numeric.p50 = "p50",
    numeric.p75 = "p75",
    numeric.p100 = "p100",
    numeric.hist = "histogram"
  ) |> 
  cols_align(
    columns = -1,
    align = "center"
  )
```

We see from @tbl-desc-data that some of the variables have missing data, specifically, the `literacym` variable has complete rate of 78%. Also, the standard deviation from each variable is in a different scale, ranging from 0.62 units in `gdp` to 38.08 units in `babymort`. This makes sense because some variables are in a log-scale.

::: {.callout-note}
Given this scenario, we would prefer to perform pca with the correlation matrix.
:::

### a)

We perform two principal component analysis using: a) the covariance matrix and b) the correlation matrix.

```{r}
#| label: pca-perform

# get complete cases
world_complete = world |> 
  drop_na()

# for the covariance matrix
pca_cov = world_complete |> 
  select(-c(1,2)) |> 
  prcomp()

# for the correlation matrix
pca_cor = world_complete |> 
  select(-c(1,2)) |> 
  prcomp(scale = TRUE)
# 
# # matrix with loadings
# loadings = bind_rows(
#   pca_cov |> 
#   tidy(matrix = "rotation") |> 
#   mutate(matrix = factor("covariance")),
#   pca_cor |> 
#   tidy(matrix = "rotation") |> 
#   mutate(matrix = factor("correlation"))
# )
```

We plot the differences for the first two pca's using each method.

```{r}
#| label: fig-plot-comparison
#| fig-cap: "Plot of principal component loadings for the first two pcs using the covariance matrix (left) and the correlation matrix (right)"
#| fig-width: 13
#| fig-height: 5

data_p01 = tibble(var = colnames(world_complete)[-c(1,2)], pca_1 = pca_cov$rotation[,1], pca_2 = pca_cov$rotation[,2]) 

data_p02 = tibble(var = colnames(world_complete)[-c(1,2)], pca_1 = pca_cor$rotation[,1], pca_2 = pca_cor$rotation[,2]) 

p01 = data_p01 |> 
  ggplot(aes(x = pca_1, y = pca_2, label= var)) +
  geom_point(size = 4, color = "red") +
  geom_label_repel(size = 4) +
  xlim(c(-0.65, 0.50)) +
  ylim(c(-0.65, 0.65)) +
  xlab("PC1 (cov)") +
  ylab("PC2 (cov)") +
  theme_classic()

p02 = data_p02 |> 
  ggplot(aes(x = pca_1, y = pca_2, label= var)) +
  geom_point(size = 4, color = "red") +
  geom_label_repel(size = 4) +
  xlim(c(-0.65, 0.50)) +
  ylim(c(-0.65, 0.65)) +
  xlab("PC1 (cor)") +
  ylab("PC2 (cor)") +
  theme_classic()

p01 | p02 
```

Based on the results presented on @fig-plot-comparison we can conclude that the loadings across the two solutions differ (although the plot only presented the first two components for each solution, the same behavior, or a lesser extent of it, is observed on the remainder of the components). The right plot shows that the variables related to `literacy` have larger values on the first and second principal component than the other variables (except for `babymort` in the second component). This behavior matches the difference in standard deviation we see in @tbl-desc-data. On the other hand, the left plot shows a more distributed pattern of the loadings, where no variable stands out from the rest. The difference between the loadings across the two solutions can be explain by the scale-dependency of the technique. Given this difference, I think it is more appropriate to analyze this dataset with the correlation matrix.

### b)

To investigate the number of necessary principal components to adequately describe the variability among the variables, several approaches will be taken.

1. Scree-plot & Kaiser rule

```{r}
#| label: fig-scree-plot
#| fig-cap: Scree plot
#| fig-width: 9
#| fig-height: 5

pca_cor |> 
  tidy(matrix = "eigenvalues") |> 
  mutate(lambda = std.dev * std.dev) |> 
  ggplot(aes(PC, lambda)) +
  geom_col(fill = "#56B4E9", alpha = 0.8) +
  geom_abline(slope =  0, intercept = 1, linetype = "dashed", color = "red") +
  xlab("PCA") +
  scale_x_continuous(breaks = 1:13) +
  scale_y_continuous(
       expand = expansion(mult = c(0, 0.01))
    ) +
  theme_minimal_hgrid(12)
```

The scree plot suggests that we need to extract two principal components, because at pca number 2 we notice the "elbow" in the amount of variance explained. After this component, the remainder pcas account for a negligible amount of variance. Plus, the Kaiser rule also suggest that we need to extract two principal components because only the first two have an eigenvalue greater than one.

2. Proportion of explained variance.

```{r}
#| label: tbl-percentage
#| tbl-cap: Proportion of variance explained (raw and cummulative) for each principal component (PC)

pca_cor |> 
  tidy(matrix = "eigenvalues") |>
  mutate(per = round(percent * 100, 2),
         per.cum = round(cumulative * 100, 2),
         .keep = "unused") |> 
  select(-std.dev) |> 
  gt() |> 
  cols_label(
    per = "Percentage",
    per.cum = "Cummulative"
  ) |> 
  cols_align(
    columns = -1,
    align = "center"
  )
```

Based on @tbl-percentage, the analysis indicates that two principal components explain over a 80% of the variance. Even thought is subjective, I believe that this percentage is good enough for this context and that two components explaining over a 80% in this analysis with 13 variables is acceptable.

::: {.callout-note}
The three methods suggest that 2 principal components appear to be necessary to describe adequately the 
variability in these variables. The rest of the analysis will be done using two principal components.
:::

### c)

PCA 1 = 

Countries that would receive high scores in this pc will be those. Countries that would receive low scores in this pc will be those

PCA 2 =

Countries that would receive high scores in this pc will be those. Countries that would receive low scores in this pc will be those

### d)

```{r}
#| label: fig-pca-01
#| fig-cap: Plot of something

world_complete = pca_cor |>
  augment(world_complete)

world_complete |> 
  ggplot() +
  geom_point(aes(x = .fittedPC1, y = .fittedPC2, color = region), size = 3) +
  xlab("PC1") +
  ylab("PC2") +
  scale_color_brewer(palette = "Dark2") +
  theme_classic()
```

### e)

```{r}
pca_cov$rotation[,1] 
```

# Question 2

We read the data

```{r}
wais = matrix(c(1.00, .37, .34, .40, .27, .59, .09, .25, .27, .22, .26,
 .37, 1.00, .27, .25, .38, .46, .10, .26, .29, .22, .24,
 .34, .27, 1.00, .36, .28, .33, .18, .32, .38, .29, .30,
 .40, .25, .36, 1.00, .22, .35, .08, .31, .26, .25, .20,
 .27, .38, .28, .22, 1.00, .29, .16, .14, .18, .15, .22,
 .59, .46, .33, .35, .29, 1.00, .08, .27, .24, .28, .26,
 .09, .10, .18, .08, .16, .08, 1.00, .19, .13, .22, .17,
 .25, .26, .32, .31, .14, .27, .19, 1.00, .36, .36, .40,
 .27, .29, .38, .26, .18, .24, .13, .36, 1.00, .30, .60, 
 .22, .22, .29, .25, .15, .28, .22, .36, .30, 1.00, .25,
 .26, .24, .30, .20, .22, .26, .17, .40, .60, .25, 1.00), 
ncol=11, byrow=T)

rownames(wais) = colnames(wais) = c("Information", "Comprehension", 
"Arithmetic", "Similarities", "Digit Span", "Vocabulary", "Digit Symbol", 
"Picture Completion", "Block Design", "Picture Arrangement", "Object 
Assembly") 
```


# Question 3